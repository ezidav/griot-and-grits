{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa1b7054-1956-4fca-b11c-abb57c7c43b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/app-root/lib/python3.9/site-packages (1.28.85)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.85 in /opt/app-root/lib/python3.9/site-packages (from boto3) (1.31.85)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /opt/app-root/lib/python3.9/site-packages (from boto3) (0.7.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/app-root/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/app-root/lib/python3.9/site-packages (from botocore<1.32.0,>=1.31.85->boto3) (1.26.18)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/app-root/lib/python3.9/site-packages (from botocore<1.32.0,>=1.31.85->boto3) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.85->boto3) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting openai-whisper\n",
      "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/app-root/lib/python3.9/site-packages (from openai-whisper) (1.24.4)\n",
      "Collecting numba\n",
      "  Downloading numba-0.60.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/app-root/lib/python3.9/site-packages (from openai-whisper) (4.66.2)\n",
      "Collecting more-itertools\n",
      "  Downloading more_itertools-10.6.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m171.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.5.1-cp39-cp39-manylinux1_x86_64.whl (906.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m130.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton>=2.0.0\n",
      "  Downloading triton-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m208.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llvmlite<0.44,>=0.43.0dev0\n",
      "  Downloading llvmlite-0.43.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m160.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.9/780.9 kB\u001b[0m \u001b[31m229.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /opt/app-root/lib/python3.9/site-packages (from tiktoken->openai-whisper) (2.31.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /opt/app-root/lib/python3.9/site-packages (from torch->openai-whisper) (4.10.0)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m128.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m147.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m151.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m165.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m172.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m159.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/app-root/lib/python3.9/site-packages (from torch->openai-whisper) (3.2.1)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton>=2.0.0\n",
      "  Downloading triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m137.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch->openai-whisper) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/app-root/lib/python3.9/site-packages (from torch->openai-whisper) (2024.2.0)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from torch->openai-whisper) (3.13.1)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m215.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m208.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (1.26.18)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803373 sha256=87ecc7669549851da093645e1911a14201041e6b58407ab463d81bc9616892a9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ife8ts3k/wheels/94/29/f3/3dd4d7f88df5d701acd3206732dcb6265379c5ece94b472c17\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: mpmath, triton, sympy, regex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, more-itertools, llvmlite, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, nvidia-cusolver-cu12, torch, openai-whisper\n",
      "Successfully installed llvmlite-0.43.0 more-itertools-10.6.0 mpmath-1.3.0 numba-0.60.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-whisper-20240930 regex-2024.11.6 sympy-1.13.1 tiktoken-0.8.0 torch-2.5.1 triton-3.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is an example notebook for using Whisper to extract text from videos.\n",
    "You can choose to run the shorter sample video, or the longer video.\n",
    "\"\"\"\n",
    "\n",
    "!pip install boto3\n",
    "!pip install -U openai-whisper\n",
    "#!pip install git+https://github.com/openai/whisper.git\n",
    "\n",
    "import boto3\n",
    "import whisper\n",
    "import os\n",
    "\n",
    "AWS_S3_ENDPOINT = os.getenv(\"AWS_S3_ENDPOINT\")\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_S3_BUCKET = os.getenv(\"AWS_S3_BUCKET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ed4353-f60f-4aac-ac0a-65f9a4961984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 40.0M  100 40.0M    0     0  12.0M      0  0:00:03  0:00:03 --:--:-- 12.0M\n",
      "env: PATH=/opt/app-root/bin:/opt/app-root/src/.local/bin/:/opt/app-root/src/bin:/opt/app-root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/mssql-tools18/bin:./ffmpeg-git-20240629-amd64-static\n",
      "\n",
      "/opt/app-root/src/ffmpeg-git-20240629-amd64-static/ffmpeg\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Install ffmpeg for video processing\n",
    "exist = !which ffmpeg\n",
    "if \"no ffmpeg\" in exist[0]:\n",
    "  !curl https://johnvansickle.com/ffmpeg/builds/ffmpeg-git-amd64-static.tar.xz -o ffmpeg.tar.xz \\\n",
    "     && tar -xf ffmpeg.tar.xz && rm ffmpeg.tar.xz\n",
    "  ffmdir = !find . -iname ffmpeg-*-static\n",
    "  path = %env PATH\n",
    "  path = path + ':' + ffmdir[0]\n",
    "  %env PATH $path\n",
    "print('')\n",
    "!which ffmpeg\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501654c4-ff71-4b70-9dbb-2440a7f4577c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the boto3 client\n",
    "s3_client = boto3.client(\n",
    "    's3', \n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID, \n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    endpoint_url='https://' + AWS_S3_ENDPOINT,\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# Specify the bucket name and file key\n",
    "bucket_name = \"griot-grits-catalog\"\n",
    "file_key_prefix = \"assets/raw/recordings/\"\n",
    "\n",
    "#This is a long video that takes significant time to process\n",
    "#file_key = \"mcduffie-sharon-20241221-1of2.mp4\"\n",
    "\n",
    "#This can be used instead of the full video depending on the available hardware.\n",
    "#The sample video is much smaller and can be processed in a couple of minutes.\n",
    "file_key = \"sample_video.mp4\"\n",
    "\n",
    "download_path = \"/tmp/\" + file_key # Local path where the file will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58758c07-4ecb-4b67-a749-0d496516ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file from S3\n",
    "s3_client.download_file(bucket_name, file_key_prefix + file_key, download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3903697-467c-4a15-a970-738dd59d73e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/opt/app-root/lib64/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " And my grandfather, like I said, he didn't play when it came to his grandchildren with us. And he came out, barely not to store. And he went walking across, he went marching across the street over there to the guys house with the dolls and the dolls. So I am coming and the dolls ran in the house after they had chased us up back down the street. And I never seen anything like, you know, he like to see them bring that in and play and them dolls. And I never saw anything like, you know, he was a little bit more than I did. And I never saw anything like, you know, he was a little bit more than I did. And I never saw anything like, you know, he was a little bit more than I did. And I never saw anything like, you know, he was a little bit more than I did. And I never saw anything like, you know, he was a little bit more than I did.\n"
     ]
    }
   ],
   "source": [
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Transcribe the audio from the downloaded MP4 file\n",
    "#use sample_video_path or download_path variable\n",
    "result = model.transcribe(download_path)\n",
    "\n",
    "# Print the transcribed text\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46795cae-1256-48c8-89fa-cd4dbc8eb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the downloaded file\n",
    "os.remove(download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2947549c-1170-464e-8a8a-1c2468db5702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
